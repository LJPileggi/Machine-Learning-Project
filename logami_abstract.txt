We developed a Multilayer Perceptron, that we trained using backpropagation on both the MONK and MLCUP datasets; we explored several configurations of both architectures and hyperparameters. The best configuration has been selected through a grid search, estimating the generalization error with multiple initialization for the MONK, and through a 4-fold cross validation for MLCUP. The performances were assessed on the MONK test sets and MLCUP holdout internal dataset.